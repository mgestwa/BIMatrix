from typing import Dict, List, Union, Any
import json
import os
import asyncio

# -------------------------------
# CzÄ™Å›Ä‡ 1. Przetwarzanie danych JSON
# -------------------------------

DESIRED_KEYS = {
    "Name", "Class", "GlobalId", "Rodzina i typ",
    "DÅ‚ugoÅ›Ä‡", "SzerokoÅ›Ä‡", "WielkoÅ›Ä‡", "WysokoÅ›Ä‡", "Typ systemu"
}

PIPE_INDICATORS = {"GruboÅ›Ä‡ Å›cianki", "Åšrednica wewnÄ™trzna", "Åšrednica zewnÄ™trzna"}

def is_pipe_element(node: Dict) -> bool:
    """Sprawdza, czy element jest rurowy na podstawie jego atrybutÃ³w."""
    return any(
        child.get("data", {}).get("Name", "").strip() in PIPE_INDICATORS
        for child in node.get("children", [])
    )

def clean_value(value: Any) -> str:
    """CzyÅ›ci i formatuje wartoÅ›Ä‡."""
    return value.strip() if isinstance(value, str) else str(value)

def process_dimensions(node: Dict, result: Dict) -> None:
    """Przetwarza wymiary elementu, uwzglÄ™dniajÄ…c specyfikÄ™ elementÃ³w rurowych."""
    is_pipe = is_pipe_element(node)
    
    dimension_mapping = {
        "DÅ‚ugoÅ›Ä‡": "DÅ‚ugoÅ›Ä‡",
        "WielkoÅ›Ä‡": "WielkoÅ›Ä‡",
        "SzerokoÅ›Ä‡": "SzerokoÅ›Ä‡",
        "WysokoÅ›Ä‡": "WysokoÅ›Ä‡"
    }

    for child in node.get("children", []):
        child_data = child.get("data", {})
        key = child_data.get("Name", "").strip()
        
        if key in dimension_mapping and key not in result:
            if is_pipe and key in ("SzerokoÅ›Ä‡", "WysokoÅ›Ä‡"):
                result[key] = ""
            else:
                result[key] = clean_value(child_data.get("Value", ""))

    if is_pipe:
        result["SzerokoÅ›Ä‡"] = result["WysokoÅ›Ä‡"] = ""

def process_node(node: Union[Dict, List], result: Dict) -> None:
    """Przetwarza wÄ™zeÅ‚ drzewa JSON rekurencyjnie."""
    if isinstance(node, list):
        for item in node:
            process_node(item, result)
        return

    data = node.get("data", {})
    node_name = data.get("Name", "").strip()
    
    if node_name == "Wymiary":
        process_dimensions(node, result)
        return
        
    if node_name in DESIRED_KEYS and node_name not in result:
        if "Value" in data:
            result[node_name] = clean_value(data["Value"])
        elif node_name == "Name":
            result["Name"] = node_name

    for child in node.get("children", []):
        process_node(child, result)

def process_single_element(element: Dict) -> Dict:
    """Przetwarza pojedynczy element JSON."""
    result = {}
    process_node(element, result)
    return result

def process_json_data(json_data: Union[Dict, List]) -> Union[Dict, List[Dict]]:
    """Przetwarza dane JSON i zwraca przetworzone wyniki."""
    if isinstance(json_data, list):
        return [process_single_element(element) for element in json_data]
    return process_single_element(json_data)

# -------------------------------
# CzÄ™Å›Ä‡ 2. Rozszerzenie: baza RAG z SemanticKernel
# -------------------------------

# Importujemy SemanticKernel oraz konfigurujemy usÅ‚ugi LLM
# Upewnij siÄ™, Å¼e masz zainstalowanÄ… bibliotekÄ™: pip install semantic-kernel
import semantic_kernel as sk
from semantic_kernel.connectors.ai.open_ai import (
    AzureChatCompletion,  
    OpenAIChatCompletion,
    OpenAITextEmbedding
)
from semantic_kernel.memory import VolatileMemoryStore
from semantic_kernel.memory.semantic_text_memory import SemanticTextMemory
from semantic_kernel.connectors.ai.open_ai import OpenAIChatPromptExecutionSettings
from semantic_kernel.contents.chat_history import ChatHistory
from dotenv import load_dotenv

load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")

execution_settings = OpenAIChatPromptExecutionSettings()
chat_history = ChatHistory()


def initialize_kernel():
    """
    Inicjalizuje jÄ…dro SemanticKernel, konfiguruje usÅ‚ugÄ™ text completions
    oraz embedding.
    """
    kernel = sk.Kernel()

    # Konfiguracja usÅ‚ugi embeddings
    embedding_service = OpenAITextEmbedding(
        ai_model_id="text-embedding-ada-002",
        api_key=api_key
    )
    kernel.add_service(embedding_service)

    # Konfiguracja usÅ‚ugi text completions
    chat_service = OpenAIChatCompletion(
        service_id="chat-gpt",
        ai_model_id="gpt-4",
        api_key=api_key
    )
    kernel.add_service(chat_service)

    # Inicjalizacja pamiÄ™ci wektorowej z embeddings_generator
    memory_store = SemanticTextMemory(
        storage=VolatileMemoryStore(), 
        embeddings_generator=embedding_service
    )
    return kernel, memory_store

# Modify add_element_to_memory to async
async def add_element_to_memory(memory_store, element: Dict):
    """
    Dodaje przetworzony element IFC do pamiÄ™ci wektorowej.
    """
    content = ", ".join(f"{k}: {v}" for k, v in element.items())
    record_id = element.get("GlobalId") or str(hash(content))
    description = f"Element IFC klasy {element.get('Class', 'Nieznana')}"

    await memory_store.save_information(
        collection="ifc_elements",
        text=content,
        id=record_id,
        description=description
    )
    print(f"ğŸ“¥ Dodano element {record_id[:8]}... do pamiÄ™ci")

# Modify build_rag_database to async
async def build_rag_database(processed_data: Union[Dict, List[Dict]], memory_store):
    print("ğŸ—ï¸ Rozpoczynam budowanie bazy RAG...")
    if isinstance(processed_data, dict):
        await add_element_to_memory(memory_store, processed_data)
        print("âœ… Dodano pojedynczy element do bazy")
    else:
        total = len(processed_data)
        for idx, element in enumerate(processed_data, 1):
            await add_element_to_memory(memory_store, element)
            print(f"ğŸ“Š PostÄ™p: {idx}/{total} elementÃ³w ({(idx/total)*100:.1f}%)")
    print("ğŸ‰ ZakoÅ„czono budowanie bazy RAG!")

# Modify query_ifc_model to async
async def query_ifc_model(kernel, memory_store, query: str, top_k: int = 5):
    print(f"ğŸ” PrzeszukujÄ™ bazÄ™ dla zapytania: '{query}'")
    search_results = await memory_store.search("ifc_elements", query, limit=top_k)
    print(f"ğŸ“ Znaleziono {len(search_results)} pasujÄ…cych elementÃ³w")
    
    context = "\n".join(result.text for result in search_results)
    prompt = (
        f"Na podstawie poniÅ¼szych danych o modelu IFC, odpowiedz na pytanie:\n\n"
        f"Kontekst:\n{context}\n\nPytanie: {query}\n\nOdpowiedÅº:"
    )
    
    print("ğŸ¤– PrzygotowujÄ™ zapytanie do LLM...")
    chat_history = ChatHistory()
    chat_history.add_system_message("JesteÅ› asystentem pomagajÄ…cym analizowaÄ‡ modele IFC.")
    chat_history.add_user_message(prompt)
    
    print("â³ Czekam na odpowiedÅº od LLM...")
    chat_service = kernel.get_service("chat-gpt")
    response = await chat_service.get_chat_message_content(
        chat_history=chat_history, 
        settings=execution_settings
    )
    print("âœ¨ Otrzymano odpowiedÅº od LLM")
    return response.content

# Convert main to async function
async def main() -> None:
    print("ğŸš€ Uruchamiam aplikacjÄ™...")
    
    try:
        print("ğŸ“‚ WczytujÄ™ plik data.json...")
        with open('data.json', 'r', encoding='utf-8') as f:
            json_data = json.load(f)
        results = process_json_data(json_data)
        print("âœ… PomyÅ›lnie przetworzono dane IFC")
    except FileNotFoundError:
        print("âŒ BÅ‚Ä…d: Plik data.json nie zostaÅ‚ znaleziony!")
        return
    except json.JSONDecodeError as e:
        print(f"âŒ BÅ‚Ä…d dekodowania JSON: {e}")
        return

    print("ğŸ”§ InicjalizujÄ™ SemanticKernel...")
    kernel, memory_store = initialize_kernel()
    print("âœ… Kernel i memory_store zainicjalizowane")

    await build_rag_database(results, memory_store)

    query = "Jaka jest dÅ‚ugoÅ›Ä‡ elementu typu IFCFLOWSEGMENT?"
    print("\nğŸ’­ PrzykÅ‚adowe zapytanie:", query)
    answer = await query_ifc_model(kernel, memory_store, query)
    print("\nğŸ“¢ OdpowiedÅº LLM:")
    print(f"ğŸ—¨ï¸ {answer}")
    print("\nâœ¨ ZakoÅ„czono dziaÅ‚anie aplikacji")

if __name__ == "__main__":
    asyncio.run(main())